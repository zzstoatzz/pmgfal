use std::fs;
use std::path::Path;

use atrium_lex::lexicon::{
    LexObject, LexObjectProperty, LexRecord, LexUserType,
};
use atrium_lex::LexiconDoc;
use heck::{ToPascalCase, ToSnakeCase};
use pyo3::prelude::*;

const HEADER: &str = r#"# auto-generated by pmgfal - do not edit

from __future__ import annotations

from typing import Any

from pydantic import BaseModel, Field
"#;

/// convert lexicon type to python type annotation
fn type_to_python(prop: &LexObjectProperty) -> String {
    match prop {
        LexObjectProperty::Boolean(_) => "bool".to_string(),
        LexObjectProperty::Integer(_) => "int".to_string(),
        LexObjectProperty::String(_) => "str".to_string(),
        LexObjectProperty::Bytes(_) => "bytes".to_string(),
        LexObjectProperty::CidLink(_) => "str".to_string(),
        LexObjectProperty::Blob(_) => "dict[str, Any]".to_string(),
        LexObjectProperty::Array(arr) => {
            let item_type = match &arr.items {
                atrium_lex::lexicon::LexArrayItem::Boolean(_) => "bool",
                atrium_lex::lexicon::LexArrayItem::Integer(_) => "int",
                atrium_lex::lexicon::LexArrayItem::String(_) => "str",
                atrium_lex::lexicon::LexArrayItem::Bytes(_) => "bytes",
                atrium_lex::lexicon::LexArrayItem::CidLink(_) => "str",
                atrium_lex::lexicon::LexArrayItem::Blob(_) => "dict[str, Any]",
                atrium_lex::lexicon::LexArrayItem::Ref(_) => "dict[str, Any]",
                atrium_lex::lexicon::LexArrayItem::Union(_) => "dict[str, Any]",
                atrium_lex::lexicon::LexArrayItem::Unknown(_) => "Any",
            };
            format!("list[{}]", item_type)
        }
        LexObjectProperty::Ref(_) => "dict[str, Any]".to_string(),
        LexObjectProperty::Union(_) => "dict[str, Any]".to_string(),
        LexObjectProperty::Unknown(_) => "Any".to_string(),
    }
}

/// generate class name from nsid and def name
fn to_class_name(nsid: &str, def_name: &str) -> String {
    let mut parts: Vec<&str> = nsid.split('.').collect();
    if def_name != "main" {
        parts.push(def_name);
    }
    parts.iter().map(|p| p.to_pascal_case()).collect()
}

/// generate python field name
fn to_field_name(name: &str) -> String {
    let snake = name.to_snake_case();
    // handle python keywords
    match snake.as_str() {
        "type" | "class" | "import" | "from" | "global" | "lambda" => format!("{}_", snake),
        _ => snake,
    }
}

/// generate a pydantic model class from an object definition
fn generate_object_class(
    class_name: &str,
    obj: &LexObject,
    description: Option<&str>,
) -> String {
    let mut lines = vec![format!("class {}(BaseModel):", class_name)];

    if let Some(desc) = description {
        lines.push(format!("    \"\"\"{}\"\"\"", desc));
    }

    if obj.properties.is_empty() {
        lines.push("    pass".to_string());
        return lines.join("\n");
    }

    let required: std::collections::HashSet<_> = obj
        .required
        .as_ref()
        .map(|r| r.iter().collect())
        .unwrap_or_default();

    for (name, prop) in &obj.properties {
        let field_name = to_field_name(name);
        let mut py_type = type_to_python(prop);

        let is_required = required.contains(name);
        if !is_required {
            py_type = format!("{} | None", py_type);
        }

        let needs_alias = field_name != *name;
        let needs_default = !is_required;

        if needs_alias || needs_default {
            let mut field_args = vec![];
            if needs_default {
                field_args.push("default=None".to_string());
            }
            if needs_alias {
                field_args.push(format!("alias=\"{}\"", name));
            }
            lines.push(format!(
                "    {}: {} = Field({})",
                field_name,
                py_type,
                field_args.join(", ")
            ));
        } else {
            lines.push(format!("    {}: {}", field_name, py_type));
        }
    }

    lines.join("\n")
}

/// parse lexicon files from a directory
fn parse_lexicons(dir: &Path) -> Result<Vec<LexiconDoc>, String> {
    let mut docs = vec![];

    fn visit_dir(dir: &Path, docs: &mut Vec<LexiconDoc>) -> Result<(), String> {
        if !dir.is_dir() {
            return Err(format!("not a directory: {}", dir.display()));
        }

        for entry in fs::read_dir(dir).map_err(|e| e.to_string())? {
            let entry = entry.map_err(|e| e.to_string())?;
            let path = entry.path();

            if path.is_dir() {
                visit_dir(&path, docs)?;
            } else if path.extension().map(|e| e == "json").unwrap_or(false) {
                let content = fs::read_to_string(&path).map_err(|e| e.to_string())?;
                match serde_json::from_str::<LexiconDoc>(&content) {
                    Ok(doc) => docs.push(doc),
                    Err(_) => continue, // skip non-lexicon json
                }
            }
        }
        Ok(())
    }

    visit_dir(dir, &mut docs)?;
    docs.sort_by(|a, b| a.id.cmp(&b.id));
    Ok(docs)
}

/// generate pydantic models from lexicon files
#[pyfunction]
#[pyo3(signature = (lexicon_dir, output_dir, namespace_prefix=None))]
fn generate(
    lexicon_dir: &str,
    output_dir: &str,
    namespace_prefix: Option<&str>,
) -> PyResult<Vec<String>> {
    let lexicon_path = Path::new(lexicon_dir);
    let output_path = Path::new(output_dir);

    let docs = parse_lexicons(lexicon_path)
        .map_err(|e| PyErr::new::<pyo3::exceptions::PyValueError, _>(e))?;

    let filtered: Vec<_> = docs
        .iter()
        .filter(|doc| {
            namespace_prefix
                .map(|p| doc.id.starts_with(p))
                .unwrap_or(true)
        })
        .collect();

    if filtered.is_empty() {
        return Ok(vec![]);
    }

    fs::create_dir_all(output_path)
        .map_err(|e| PyErr::new::<pyo3::exceptions::PyIOError, _>(e.to_string()))?;

    let mut output = String::from(HEADER);
    output.push('\n');

    for doc in &filtered {
        output.push_str(&format!("\n# {}\n", doc.id));

        for (def_name, def) in &doc.defs {
            let class_name = to_class_name(&doc.id, def_name);

            match def {
                LexUserType::Record(LexRecord { record, description, .. }) => {
                    let atrium_lex::lexicon::LexRecordRecord::Object(obj) = record;
                    let desc = description.as_deref().unwrap_or(&doc.id);
                    output.push_str(&generate_object_class(&class_name, obj, Some(desc)));
                    output.push_str("\n\n");
                }
                LexUserType::Object(obj) => {
                    output.push_str(&generate_object_class(
                        &class_name,
                        obj,
                        obj.description.as_deref(),
                    ));
                    output.push_str("\n\n");
                }
                LexUserType::Token(_) => {
                    output.push_str(&format!(
                        "# token: {}\n{} = \"{}#{}\"\n\n",
                        class_name,
                        class_name.to_uppercase(),
                        doc.id,
                        def_name
                    ));
                }
                _ => {}
            }
        }
    }

    let output_file = if let Some(prefix) = namespace_prefix {
        output_path.join(format!("{}.py", prefix.replace('.', "_")))
    } else {
        output_path.join("models.py")
    };

    fs::write(&output_file, &output)
        .map_err(|e| PyErr::new::<pyo3::exceptions::PyIOError, _>(e.to_string()))?;

    Ok(vec![output_file.to_string_lossy().to_string()])
}

#[pymodule]
fn _pmgfal(m: &Bound<'_, PyModule>) -> PyResult<()> {
    m.add_function(wrap_pyfunction!(generate, m)?)?;
    m.add("__version__", env!("CARGO_PKG_VERSION"))?;
    Ok(())
}
